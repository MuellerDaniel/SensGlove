
\lhead[\chaptername~\thechapter]{\rightmark}

\rhead[\leftmark]{}

\lfoot[\thepage]{}

\cfoot{}

\rfoot[]{\thepage}


\chapter{Related Work}
\label{cha:relatedWork}

%\section{Abstracts of papers}
%\begin{itemize}
%\item \cite{huang2008pianotouch}
%\item \cite{huang2010mobile}
%\item \cite{metcalf2013markerless}
%\end{itemize}

\section{Approaches for hand motion reconstruction} \label{sec:approaches}

\cite{sturman1994survey}, \cite{dipietro2008survey}

\subsection{Vision/Camera based} \label{subsec:approaches:vision}
\todo{Review the papers again! Where is the accuracy mentioned? When it is not mentioned, then don't say sth about it...\\}
% General part about vision based motion estimation
Vision based motion capturing systems are widely used. They consist of one ore more cameras, arranged in a certain configuration, to generate a quite exact replication of what wants to be tracked. Those systems are nowadays not only used for tracking and analyzing the motion of humans. The systems and applications range from general purpose devices for entertainment, like interacting with video games or for examining the movement of athletes \cite{zhang2012microsoft}, \cite{boyd2012situ}. So in the end it is no wonder that some groups came up with using a vision based system for hand motion reconstruction, even though those movements bring in some challenging aspects to consider. However the quality of the results of such vision based systems is very high and is often classified as ground truth for motion estimation of fingers. The Optotrak system, which is used by several groups, for example has an accuracy of up to \SI{0.1}{\mm} with an resolution of \SI{0.01}{\mm} \cite{optotrak}. It is very hard to manually reconstruct and measure the real values of finger angles and hand motion, since one can not see the bones without an x-ray. \\
% Steps and 3d model description
No matter what kind of vision based motion tracking system is used, to extract the actual hand pose and movement from an image, one has to perform the following steps:
\begin{enumerate}
\item Image acquisition, fusion (if more than one camera is used) and preprocessing
\item Image processing, to get a focus on the relevant sections (the region of interest - ROI) 
\item Pose estimation, to extract and calculate the actual body, respectively hand pose from the image
\end{enumerate}
A fundamental part for these steps is providing a proper three dimensional object model. No matter whether one tracks the whole body or only a small section, the more detailed the outcome of the system should be, the more detailed has to be the model. The model uses a mesh of triangles and vertices and applies (anatomical) constraints on them. After extracting the relevant sections from the camera image, the model calculates and maps the actual positions and relations between the joints and bones. This step can consume a lot of computation time, if the result should be as detailed as possible. Yun et al. for example solved this problem, by combining a system identification stage, which uses the hand model, with a state estimation stage, where an Extended Kalman Filter is used \cite{yun2013accurate}.

For reconstructing the hand motion with a vision based system, one can find two approaches in literature: The tracking of markers, placed on the hand or a textile glove and the markerless detection of palm and fingers. At first a short overview on the marker based systems is presented. \\
% Marker approach
Supuk et al. \cite{supuk2008evaluation} and Metcalf et al. \cite{metcalf2008validation} use passive reflective markers attached to the hand. While the Optotrak system \cite{optotrak}, used by Supuk et al., only needs one camera instance, the Vicon system \cite{vicon} consists of at least two and can handle up to six cameras for more accuracy. Comparing the effort and capabilities of those systems, measuring only the movement of small hands seems to break the relations. The accuracy of the outcome is directly dependent on the number and positions of used markers. Unfortunately there are no exact numbers given about the achieved accuracy of the vision systems. Metcalf et al. modeled the movement of wrist, hand, fingers and thumb. Therefore they compare the results for different test persons, each equipped with 26 markers in total for one hand. The passive stickers are placed at the three knuckles of each finger, the fingertips and on the back of the hand and lower forearm to guarantee a tracking of the whole hand motion and not only of the fingers (see figure\ref{fig:markers}). It is very important, to place the markers for each person at the same anatomical positions. Attaching the reflectors statically on a textile glove would make the system more flexible and easier to use, but this would also lead to a degradation of the results. Every person's hand varies not only in size, but also in the position and lengths of the individual bones and knuckles. So a general purpose glove is very hard to construct. One additional issue that Metcalf et al. came up with is, that one has to take the size of the surface area into account, too. The hands of children for example won't have the surface to place all markers at the desired positions properly. The placement of the stickers took them between three and five minutes each time. In fact the aim of their study was to show that persons perform specific tasks in their own way, but that one can still observe similarities. Supuk et al. used the camera system more or less only as ground truth, to validate the data from a flexion based Data Glove. They use 19 passive markers, placed in a similar shape than the other group. The group of Yun et al. are using active LED markers. Their paper emphasizes on an effective system identification algorithm and filtering method. They estimate one index finger with seven markers on it, recording it with a system from Phasespace Inc. The accuracy of the measurements was verified by comparing the results to an optimized kinematic model. Here again, no exact numbers about are accuracy are provided.\\
The group of Wang et al. uses a multi-colored glove for finger identification. Their glove is printed with a special color pattern, to simplify the pose estimation problem. This allows them to use one general purpose color camera, which is way cheaper than the motion capturing systems mentioned beforehand. A setup of their system is shown in figure\ref{fig:color}. The pose estimation is done upon a database, containing the glove in different articulations. The image gets processed, to extract the colors clearly, and the pose is found by a nearest neighbors approach, comparing to the database. To penalize the difference between the image and the matched pose from the database, they tune the result by applying inverse kinematics. In the end their systems works reliably. But it is only applicable for one glove size and the results are based on a single test person.\\

For all the above approaches it is very important to place the markers at the correct anatomical positions, to get good and reliable results. So the system presumes that the user knows how to attach the stickers or wear the glove and which mistakes can be made. In order to facilitate this process and make it less fault-prone, a markerless approach would be a better choice.\\
By realizing such a variant however, the region of interest, so to say the actual position of the hand, is not directly given. Further on, the orientation and alignment of the test person has to be interpreted. The following section presents some implementations.\\
For image acquisition Ionescu et al. use a gray scale camera and filter the image for the biggest white region. To get the best results they mention that one has to hold the hand in front of a black surface. In the end, they only try to detect certain hand gestures and not a whole motion. The images are compared to a pre-learned training set in order to recognize the poses. So this group doesn't use any models.\\
Metcalf et al. and Sharp et al. use the Microsoft Kinect. This system defines anatomic landmarks, to identify certain points of the hand. Actually the sensors are designed for declaring landmarks on the whole body, so for tracking a petite hand this identification process has to be adopted. Metcalf et al. use the binary depth image and define the landmarks by searching for reasonable maxima and minima in combination with a 3d hand model. Possible poses of the hand are simulated with the model and adopted to the image. In the end their approach led to an overall accuracy of \SI{78}{\percent}. A marker based system served as a comparison. The approach of Metcalf et al. was only tested with persons sitting on a table, so it is designed for a front-facing close-range scenario.  The group of Sharp et al. goes one step further and brings the system to a universal surrounding. They are able to extract the hand posture and movement from an arbitrary image, no matter at which distance the hand is or what the background looks like. The approach is to introduce a robust reinitializer to handle typical vision based problems like occlusion and image loss. In combination with a fast and effective comparison to a 3D hand model and a learned training data set, the movement and pose of the hand can be estimated very reliably, independent from the person or the environmental circumstances.\\
John et al. use two high resolution color cameras from Sony. To reconstruct the motion of the extracted human hand, the system compares the images to data from the 3d hand model. By matching the model to the input pictures, the position and configuration of the hand is estimated in real time.\\
The commercially available Leap Motion system \cite{leap} includes two IR cameras and three IR emitters and is particularly constructed for hand motion reconstruction. The small controller just has to be put under your hands, like shown in figure\ref{fig:leap}. The Leap Motion Inc. provides a well documented API and software tools for Windows and Mac systems to use their device for basic interaction with a PC. The system directly outputs hand- and fingerpositions. It can also detect whether one is holding a pen or specific tool. Via the API one can directly access the absolute positions of the hand and joints. Also specific gestures, like swiping or drawing a circle with a finger gets directly detected. The accuracy and robustness of the Leap system is analyzed in \cite{weichert2013analysis}. Different motions and positions were examined. To ensure a reliable \grqq test person\grqq~ they choose an industrial robot with a position accuracy of \SI{0.2}{mm}. The overall error of the system for dynamic motions was below \SI{0.7}{\mm}, which is better than the Microsoft Kinect system.\\
The Digits system, developed by Chang et al., is a wrist wearable IR sensor \cite{Digits}. It consists of an IR laser line generator, a ring of modulated IR LEDs, a IR camera, and an IMU (see figure\ref{fig:digits}). The system collects on the one hand a single 3D point for each finger from the line generator, on the other hand a uniformly illuminated image of the hand, produced by the modulated IR LEDs. With those informations and by using inverse kinematics of the underlying human hand model, the group can robustly reconstruct inward hand and finger movement. The IMU can track the alignment and movement of the whole forearm. The overall angular error of the system is~ $ \leq \ang{9}$~ for the joint angles. This value varies with the fingers, since the thumb has a more complex movement and is smaller than the index finger for example. However these values satisfy the clinical standards for joint measurement.\\

Concluding the presented techniques for hand motion reconstruction by a vision based system, one could assert the following basic characteristics of such systems. (The impact or applicability of each point varies with the system, of course):
\begin{itemize}
\item The quality and stability of the tracking is limited to light conditions.
\item Occlusion of unseen fingers, for example by crossing or making a fist, can occur. Also clothes or body parts, held in front of the camera can hide parts of the hand.
\item The proposed systems are usually quite big or even need multiple cameras.
\item This induces that the installation has to be static (like depicted in figure\ref{fig:setup}) and is only capable of localisation based tracking.
\item A three dimensional model of the human hand gets adopted to the images.
\item The algorithms for motion tracking are quite complex and are running on an external PC.
\item For marker based systems: The placement of the markers is crucial.
\end{itemize}

\begin{figure}[hp]
	\centering
	\subfloat[The positions of the markers, used in \cite{metcalf2008validation}]
	{\includegraphics[width=0.4\textwidth]{pictures/marker.png}\label{fig:markers}}
	\hfill
	\subfloat[The Color glove with the used camera setup \cite{Wang:2009:RTH}]
	{\includegraphics[width=0.4\textwidth]{pictures/color.png}\label{fig:color}}
	\hfill
	\subfloat[Example for a static camera - subject setup, used by \cite{metcalf2013markerless}]
	{\includegraphics[width=0.4\textwidth]{pictures/staticSetup.png}\label{fig:setup}}
	\hfill
	\subfloat[The Leap Motion system \cite{leap}. The hands get directly visualized on the screen.]
	{\includegraphics[width=0.4\textwidth]{pictures/leap.jpg}\label{fig:leap}}
	\hfill
	\subfloat[The Digits system \cite{Digits} with the relevant parts.]
	{\includegraphics[width=0.45\textwidth]{pictures/digits.png}\label{fig:digits}}
	
	\caption{Some examples of the described vision based systems}
	\label{fig:examplesVision}	
\end{figure}

\newpage

\subsection{IMU based} \label{subsec:approaches:IMU}
Another concept of motion tracking consists of using IMUs. Those sensors measure the angular rate, acceleration and magnetic field for three dimensions in space, they are also called 9-DOF sensors. With existing suitable algorithms, like a Madgwick filter \cite{madgwick2010efficient} one can calculate the absolute orientation of the sensor, relative to the earth magnetic field, from the sensor data. So one can track instantaneous the orientation of a sensor unit. Therefore it is no surprise that those sensors are commonly used for motion tracking applications in general. The Dutch company Xsens Technologies, for example is specialized on motion tracking using IMUs and develops several suits to track the whole body motion.\\
Kortier et al. use a self designed IMU system, consisting of 18 sensor units in total. The sensors are placed on the bare hand, like depicted in figure\ref{fig:imu}. The units are a gyroscope-accelerometer combination and placed on each proximal, intermediate and distal phalanges of each finger (for explanation of the bones see \ref{sec:anatomy}). For additional information three units are placed on the back of the hand. The PCBs on the fingertips and on the dorsal side are additionally equipped with a magnetometer, to get a more accurate estimation about the orientation of the hand. To filter, estimate and map the raw sensor data to an adequate biomechanical hand model the group uses an Extended Kalman Filter (EKF) framework. They achieved an adequate repeatability. By comparing their approach to a vision based one, a maximum error of \SI{12.4}{mm} was achieved. This value seems pretty high, but they claim that it is because of an misalignment between the optical and their own chosen coordinate frame. This shows once again, that the calibration plays an important role for vision based systems and is not so easy to manage.\\
The group of Fang et al. uses a similar method than Kortier et al. The 16 IMUs, which are all full 9-DOF units, are also placed on the three bones of each finger. For the palmar movement however they use only one sensor. The processing of the data is also done ~\grqq on-hand\grqq~ with the self designed processor-board. For data filtering and position estimation they also use a Kalman Filter and a hand model. The characteristic of the approach lies in the evaluation of the sensor values. Because the hand is composed of rotational joints, they assume that either all sensors are in rotation or none. So they neglect the measurements of the gyros, if the hand is held still and only the fingers are moving, such that they only take the accelerometer and magnetometer data into account. However when the hand is moving, the measurements of the accelerometer and magnetometer have lower dynamics and they use the advantage of the gyros. Further on, they first estimate the pose of the palm, then the attitude of the proximal finger bones, then the angles of the DIP and PIP and finally they calculate the full hand pose, based on the intermediate results. In the end they achieved the intended requirements and point out that the efficiency of their method is almost twice that of the original EKF. Unfortunately they don't provide exact numbers.\\
There also exist some commercially available IMU based glove systems. The company Synertial or Anthrotronix for example provide ready to use gloves. The IGS-Glove from Synertial comes in various editions (two of them shown in figure\ref{fig:synertial}), differing in the number of sensors. It is available with 7, 12 or even 15 IMUs, mounted on the easy to wear glove, delivering you the desired accuracy \cite{Synertial}. Anthrotronix however equip their ''Acceleglove'' with 6 IMUs \cite{anthrotronix}. Both deliver their systems with a SDK to have direct access to the raw sensor data but also to pre-calculated motion and gesture data.\\

Again, all the presented systems show some similarities. The following points try to summarize them:
\begin{itemize}
\item The IMUs are mounted on a textile cloth
\item However a unified cloth, that fits every human hand is difficult to produce
\item The accuracy varies with the number, position and measurement range of the sensors.
\item The more sensors used, the more wires are needed. Also the data traffic and processing time increases with the number of units.
\item A calibration procedure is needed, to increase the accuracy.
\item IMUs are cheap and available in a large variety
\end{itemize}

\begin{figure}[h]
	\subfloat[In \cite{kortier2014assessment} the IMUs are placed on the bare hand.]
	{\includegraphics[width=0.4\textwidth]{pictures/imu.png}\label{fig:imu}}
	\hfill
	\subfloat[The glove system, sold by Synertial]
	{\includegraphics[width=0.4\textwidth]{pictures/synertial.jpg}\label{fig:synertial}}
	
	\caption{Examples of glove systems, using IMUs}
	\label{fig:examplesIMU}
\end{figure}

\subsection{Flexion based} \label{subsec:approaches:flexion}
Another approach of measuring the hand movement is to monitor the flexion of fingers. There are different kinds of flexion sensors out there and many researchers use them for finger tracking. For example in 1977 Thomas de Fanti and Daniel Sandin developed one of the first data glove prototypes at the Massachusetts Institute of Technology (MIT). The Sayre Glove  \cite{sturman1994survey}. They equipped a glove with flexible tubes for each finger. At one end of each tube, they put a LED as light source and at the other end a photocell. The amount of light, arriving at the sensor varies with the flexion and extension of the finger. The more the finger is bent, the less light comes to the sensor.\\
Ten years later, in 1987 Visual Programming Language Research, Inc. rolled out some kind of successor to the Sayre Glove. Their device is equipped with five to ten flexion sensors, based on optical fibre \cite{zimmerman1985optical}. For more accuracy they place a flex sensor on each joint, to measure its angle. They even proposed a system with more sensors, to measure abduction and adduction between adjacent fingers.\\
Another way to measure the flexion are resistive or capacitive bend sensors. These devices can be printed with resistive ink and are therefore highly customizable in shape and size. Resistive bend sensors are used for example by O'Flynn et al., Zecca et al. or by the company 5DT (for representative pictures see\ref{fig:examplesFlexion}). The Didjiglove in contrast is based on capacitive bend sensors \cite{sturman1994survey}.\\
The Italian company Gloreha \cite{Gloreha} follows a more application specific approach. Their rehabilitative glove system consists of mechanical cables for each finger. You can measure how much a finger is bended by the amount of extended wire. On the other hand you also can support the patient by extending or contracting the wire mechanically. This system is big, unhandy and looks more like an exoskeleton, than an unimpressive wearable. Of course it is constructed for rehabilitation and aimed to support specific motions of a patient and not for general purpose measuring of flexion and extension in every day life (more about it in \ref{subsec:applications:reha}). But it still shows a mentionable approach.

In the end, one can say that flexion based hand tracking has the following characteristics:\\
\begin{itemize}
\item The sensors are mounted on the joints. Most groups use therefore a textile glove. 
\item The output of the system is dependent on the positions of the sensors. Ideally this should not change with the user. However each human hand is slightly different and there is not a universal glove size and sensor positioning, which would fit for all.
\item One way to improve this is to calibrate the glove system for each user.
\item The accuracy of the reconstructed finger positions or gestures is limited to the number and the measurement range of the used sensors. With one bend sensor per finger, one could at most only reconstruct the intention of the user's gesture or distinguish between several postures. However by introducing multiple sensors per finger, ideally more than one per joint, one could get an acceptable result. \cite{zecca2007development} used 15 bend sensors on a flexible PCB and reached an average error $ \ang{7.1} $  compared to a camera system.
\item One measures only the bending of a joint or finger. In order to reconstruct a relative or absolute position of the finger additional calculations have to be made.
\item It is a simple and highly customizable system.
\item Easy applications can be realized with only a few sensors (see \ref{sec:applications} for more information)
\end{itemize}

\begin{figure}[h]
	\subfloat[The flexible PCB, used in \cite{o2013novel}]
	{\includegraphics[width=0.4\textwidth]{pictures/flexPCB.png}\label{fig:flexPcb}}
	\hfill
	\subfloat[The glove system, devolped in \cite{zecca2007development}.]
	{\includegraphics[width=0.4\textwidth]{pictures/flexionWB.png}\label{fig:zecca}}
	\hfill
	\subfloat[The Gloreha system for rehabilitation.]
	{\includegraphics[width=0.4\textwidth]{pictures/gloreha.jpg}\label{fig:gloreha}}	
	
	\caption{Examples of glove systems, using flexion based approaches}
	\label{fig:examplesFlexion}
\end{figure}


\subsection{Magnetic/electromagnetic based} \label{subsec:approaches:magnetic}
Another approach is the use of measuring active and passive magnetic fields.\\
Hashi et al. are using an active, resonator based system. Their system consists of a driving coil, a pick up coil array and resonant LC markers. The markers, consisting of an inductive coil and a chip capacitor, are placed on the fingertips and have different resonant frequencies. The exciting coil modulates several signals and sends them out. An electromagnetic field is generated around the coil. Now, holding a marker inside this field the electromagnetic circuit begins to oscillate and generates his own resonant electromagnetic field. This can be measured by the pick up coil array. Each marker has a unique excitation frequency, so by modulating the received signals via FFT, the markers can be identified. Further on, the measured amplitude of the signal represents the intensity of the magnetic field. The group assumes that the excited field of the marker behaves like a magnetic dipole field. So by using the suitable equation \todo{add reference to magnetics section and formula!} one can calculate the position and orientation of each marker uniquely. They tested their approach and came up with a position accuracy up to 2 mm, for locations up to \SI{100}{mm} away from the pick up coil array. Increasing the distance further, the results get worse. The group of Schaffelhofer et al. tested the commercially available system of Northern Digital \cite{wave} with primates. They achieved an overall accuracy of $ \ang{2.41} \pm \ang{3.36} $ for the tracking of dynamic movements. However the system, consisting of very complex and bulky components is not so well suited for mobile or general purpose use.\\
The group of Ma et al. take the approach of determining the position of a passive cylindrical bar magnet by approximating it with the magnetic dipole field. They tried to reconstruct the movement of the fingers, by placing neodymium magnets on the fingertips and measuring the magnetic field. A draft of the system can be seen in\ref{fig:passiveMag}. Like the active approaches, they use the model for the magnetic dipole, to estimate the position and orientation of the passive magnets. To conclude from those estimated values to the actual finger position, they use inverse kinematics with an underlying human hand model. For verification of this approach they equipped a test person with one magnet on the index fingertip and six sensors on a wristband. The proband was asked to perform several flexion and extension tasks of his finger. The results for the estimated finger positions and orientations were consistent with the data recorded by a Vicon system. Exact accuracy values are not provided by the group.\\
In the end one can say, that this approach sounds promising. 

\begin{figure}[h]
	\subfloat[A draft of the passive magnetic system, proposed in \cite{ma2011magnetic}]
	{\includegraphics[width=0.4\textwidth]{pictures/magnetic.png}\label{fig:passiveMag}}
	%\hfill
	%\subfloat[The glove system, devolped in \cite{zecca2007development}.]
	%{\includegraphics[width=0.4\textwidth]{pictures/flexionWB.png}\label{fig:zecca}}
	
	\caption{Examples of hand tracking systems, using magnetic/electromagnetic approaches}
	\label{fig:examplesMagnetic}
\end{figure}


\subsection{Other approaches} \label{subsec:approaches:other}
As one can see, there is a lot of research going on in the area of motion estimation for the human hand. The focus of the so far presented approaches was relied to general purpose devices, designed for trying to track reliable and accurate the whole range of motions. Due to the wide range of possible applications (see~\ref{sec:applications}) there are also some more specialised variants in reconstructing gestures or movements. This section introduces some of them.\\
The Pinch Gloves, visualized in\ref{fig:pinch}, designed by Fakespacelabs represents an input device \cite{bowman2001using}. The system consists of a glove and conductive elements, sewn into the tips of each of the fingers. When two or more fingers are pinched together, the conductive parts come into contact and generate an individual signal. This signal can easily be interpreted by a computer and therefore serve as an input. There is also the possibility to attach a position tracker and add the motion of the hand as an input possibility. The system is used for virtual environment interaction. In \cite{bowman2001using} a more elaborated interaction technique with this simple system is presented. They developed an environment to navigate through menus or to type on a virtual keyboard.\\
The eRing, developed by Wilhelm et al. is another kind of gesture interaction device. It consists, as the name suggests, of a ring, enclosed by capacitive foils (a first prototype is presented in\ref{fig:ering}). The capacity of the system is related to the conductive environment. The human body has an influence on the magnitude of capacity, which changes by moving the fingers around the ring. This change in capacity can be measured, by determining the rise time $ \tau $ of the RC circuit. In their paper the group describes that the system is able to detect static and dynamic gestures, as long as the neighbouring fingers are not to far away from the ring. For recognizing the gestures they use a 1-nearest neighbour approach on a pre learned dataset.\\
The Rutgers master \rom{2} represents an exoskeleton like approach \cite{bouzit2002rutgers}. It consists of four pistons with rings on the ends, to clip them on the thumb, index, middle and ring finger (see\ref{fig:rutgers}). The movement of the pinky finger is neglected. The adduction and abduction of the fingers is measured by Hall-effect sensors, the flexion and extension via infrared sensors. From the piston movement one can calculate the finger angles via a kinematic hand model. The pistons are inside an air cylinder to reduce friction in the system. This glove can provide force-feedback to the fingers, since the pistons can be controlled externally. Its main application area is therefore the rehabilitation and learning of hand movements. However the exoskeleton structure restricts the range of movement. Only \SI{55}{\percent} of the natural grasping motion can be performed with this system. The accuracy of the system was evaluated to $ \ang{0.75} $ for the adduction/abduction and \SI{0.5}{mm} for the piston position. A similar system is the CyberGrasp haptic glove \cite{cyberglove}. It is slightly more accurate (resolution of \ang{0.5}) but also heavier and even more cumbersome.\\
An interesting approach is mentioned by Mascaro et al. They introduce a sensor placed onto the fingernail, measuring emitted light. LEDs are placed on top of the nail, emitting light with different wave lengths and measure the response from the nailbed. This technique is called reflectance photoplethysmography. If there is a force applied to the fingertip, one can see that the color of the skin beneath the nail changes. This behaviour can also be obtained when moving the finger. Those, often very small and not clearly visible, changes in color can be detected by their system. However there are many factors to take into account like skin color, blood flow in the fingers, texture of skin and so on, which complicate the reconstruction of motion. Hence they are only able to measure forces with this system \cite{mascaro2001photoplethysmograph}. The draft\ref{fig:nail} illustrates their approach.\\
Another approach for recognizing gestures is by measuring the electric potential of forearm muscles \cite{kim2008emg} By performing gestures with the hand, the electromyographic potential especially in the forearm changes. This can be recorded by a so called electromyograph (EMG). This approach is based on a learning data set, recorded for one person and by recognizing those gestures in real time. Zhang et al. designed a framework for this, taking also the data of an accelerometer into account.

\begin{figure}[h]
	\subfloat[Pinchglove system, used in \cite{bowman2001using}]
	{\includegraphics[width=0.4\textwidth]{pictures/pinchgloves.jpg}\label{fig:pinch}}
	\hfill
	\subfloat[First prototype of the eRing \cite{wilhelm2015ering}]
	{\includegraphics[width=0.4\textwidth]{pictures/ering.png}\label{fig:ering}}
	\hfill
	\subfloat[The Rutgers Master \rom{2} \cite{bouzit2002rutgers}]
	{\includegraphics[width=0.4\textwidth]{pictures/rutgers2.png}\label{fig:rutgers}}
	\hfill
	\subfloat[The photoplethysmography sensor approach of \cite{mascaro2001photoplethysmograph}]
	{\includegraphics[width=0.4\textwidth]{pictures/nail.png}\label{fig:nail}}
	
	\caption{Collection of various approaches for hand motion reconstruction}
	\label{fig:examplesOther}
\end{figure}

\section{Possible fields for applications} \label{sec:applications}

\subsection{Human-Computer-Interface (HCI)} \label{subsec:applications:HCI}
The way we interact with electronic devices becomes more and more natural and is still changing. In the recent years touch input became ubiquitous, for example. Gaming consoles already bring the possibility to physically interact with games. The Kinect camera for Xbox, the Nintendo Wii controller or other haptic systems. Hand motion based systems could be one way to bring the human computer interaction to the next level. Commercially available devices like the Leap Motion or the Myo wristband allow a broad gesture based interaction with the computer or smartphone. The Leap controller for example enables you to play games or interact with 3D graphics in a natural way \cite{leap}. The benefit of such hand tracking devices is, that you don't need a big camera set up and don't have to leave your desk position. You just place the device in front of you or wear it, perform the desired gesture and the system behaves as you want. For example by swiping, a window gets closed or the field of view gets enlarged by pinching two fingers. For the interaction with mobile devices, such as smartphones, the system has to be wearable, like in \cite{Digits}. Also the evolving field of virtual reality environments serves as a base for hand motion interaction. For the Leap motion again, there exists a mount to combine it with the Oculus Rift system. Also creative tasks can be performed by a hand motion tracking system, for example drawing and designing a 3D object with your fingers on a virtual canvas.\\
Another, a bit more serious, way for HCI is the control of robotic machines, especially arm like devices. Such an application could be safety critical, so the interpretation of the hand motion has to be accurate and reliable, like the system proposed by Sharp et al. Dependent on the application, those devices should be able to track the whole finger movement and not only react on predefined gestures. Examples could be controlling robots in space \cite{dipietro2008survey} or in dangerous environments like military territory or for bomb disposal \cite{greenleaf1996developing}. Also the execution of surgical tasks could be one field of application. Nowadays invasive operations should be performed in no time and leave very small scars. For that a lot of endoscopic surgery is done with remote controlled catheters. However most of those invasive devices have very low functionality, often limited to cutting or exhausting something. The devices are controlled by the surgeon often by a simple mechanical system. Giving the practitioners the ability to use a more sensitive and complex method of interaction, would not only reduce the time needed for the intervention and the risks for the patient, but also enlarge the possibilities of interventions.\\
\todo{A concluding sentence??? Saying sth. like \grqq... one can see that the accuracy and mobility is dependent on the application...\grqq. But this is obvious and in some sense stated above...}

\subsection{Therapeutic/Rehabilitation} \label{subsec:applications:reha}
The exact measurement of finger joints, called goniometry, is still a time consuming and error-prone task. Therapists and doctors have to measure patient's range of movement (ROM) in case of hand immobility diseases like arthritis, rheumatism \cite{o2013novel} or parkinson's disease \cite{su20033}, or after a hand operation and fractions. Till now they use mechanical goniometers \cite{williams2000goniometric} to measure static angle positions, dynamic measurements are not possible to evaluate. A more accurate and reliable method would ease their life tremendously. This is where the hand motion reconstruction comes in. As described in~\ref{sec:approaches} there are already systems which can measure the ROM of fingers very accurate. Williams et al. verified their flexion based glove as clinically admitted and can even measure adduction/abduction more exactly than with goniometers. The group of O'Flynn et al. developed a flexion and IMU based glove with a very detailed interface for doctors. They can record and visualize raw sensor data, like the bending angle or movement velocity, as well as a 3D hand, miming the actions of the user. The interface can also provide information about former measurements and can therefore monitor the development of the patients rehabilitation process. The diagnosis and measurements for Parkinson's disease are also critical. Till now the diagnosis relies on objective observations by the doctor and patients. This induces that the illness is often detected in an advanced stage. Su et al. developed a system especially to detect this neurological disease. With the help of an electromagnetic based glove, several experiments which could indicate Parkinson were carried out. In the end they compared the results for ill and healthy patients and could clarify significant differences in the execution of the tasks.\\
Another possibility to support especially the work of therapists is the home-based rehabilitation or telemedicine \cite{metcalf2013markerless}. This means that the patients have a suitable guidance system at home and can perform the exercises with it. The paper of Durfee et al. validated that such systems can return the same results as clinical tests and could therefore save the patients time and effort. The system could for example detect over- or under-exertion while performing a presented task. Further on, by providing an attractive interface like a virtual-reality environment, the patients are much more motivated to execute the training on their own \cite{popescu2000virtual}. Such an interface doesn't has to be complex. The patient could see for example virtual objects on a screen and try to interact with them. The environment, consisting of a PC and the glove system, records not only the performed motions but also takes a video of the exercise session and can automatically be transmitted to the therapist. This ensures a constant verification and interaction with the clinic personal. That such a system can improve the work of therapists is validated by Heuser et al. In their study five postsurgery subjects suffering from Carpal tunnel syndrome were trained to perform tasks. The effects in hand function improvement was tremendous for all subjects. The strength for grip and pinch movement increased up to \SI{150}{\percent}. The system was very good accepted by the patients and the therapists. Such therapeutic system can benefit from approaches with force feedback. The Rutgers Master \rom{2}, CyberGrasp or the Gloreha approaches, all mentioned beforehand, are such systems. Force feedback systems are often more bulky than the motion tracking ones, in \ref{fig:gloreha} the exoskeleton like Gloreha system is pictured. Popescu et al. \cite{popescu2000virtual} for example developed a system for the hand, using the Rutgers Master \rom{2}.\\ 
However for such a telemedicine system the accuracy has to be exact and reliable, also the data has to be processed in real time. One of the most important requirement is however the usability. The system has to be easy to set up and able to detect false focuses or attitudes. A glove based system for example brings in difficulties in donning and doffing \cite{metcalf2013markerless}, other systems are cumbersome or have fragile parts like wires \cite{bouzit2002rutgers}, which could complicate the usage. For camera based systems the focus, illumination and the background of the image are critical points \cite{ionescu2005dynamic}.\\
Till now only the mentioned studies and systems, described in \cite{heuser2007telerehabilitation} and \cite{popescu2000virtual} where performed, but certainly there will be more investigation in the near future.\\


\subsection{Activity and Gesture tracking} \label{subsec:applications:activity}

As already depicted in \ref{subsec:applications:HCI}, the reconstruction of human hand motion can be used for gesture recognition. Activity recognition is mainly based on using a training data set for the gestures and an algorithm, to compare the actual movement with this database to finally judge whether the gesture has been performed or not. But this feature can not only be used as an HCI. Having a wearable system, one can detect specific gestures ubiquitously, which can be used to produce a diary like tracking of hand gestures. Such a monitoring system could be a support again for therapists, trying to analyse the daily routines of their patients. It is possible to detect a grasp intention \cite{supuk2008evaluation}, \cite{zhang2011framework} and also the strength of a grasp \cite{ekvall2005grasp}. By downsizing the systems and making them wearable one could get a detailed logbook about hand activities. The eRing \cite{wilhelm2015ering} could be such a system. With an unimposing data glove system, like the one of 5DT or Synertial, one could even track the whole hand motion. Because of the mentioned reasons to use such a system in a ubiquitous environment, vision based systems are not so well suited for this application field. Apart from the Digits \cite{Digits} system, most of the other developments use a bigger, stationary camera system.\\
Another possible application field concerning the recognition of gestures is the understanding and translation of sign language. Deaf persons are using the sign language to communicate with the outside world. However most of the~\grqq talking\grqq~persons do not know this complicated alphabet, which is based on specific hand gestures and poses, such that it can be very tiring for a deaf person to interact with others. Several groups have done research in this field like presented in \cite{mehdi2002sign}, \cite{fels1993glove} and \cite{dipietro2008survey}. Again, a wearable and mobile system is better suited for this task than a static, camera based approach. As the sign language has a lot, but sometimes similar looking gestures the training data set and the corresponding classification algorithm have to be fast and reliable. In the end such a hand tracking system has to be reliable, accurate, and able to track the movement of the whole hand. As a classification algorithm Fels et al. propose use neural networks. The detected letters or words can then be visualized on a display or directly made audible by speakers \cite{fels1993glove}. To ensure a natural behaviour, the gestures have to be interpreted and visualized in real time. Fels et al. used a fibre optic data glove with 11 sensors, including an IMU to track the orientation of the hand. In the end they achieved an accuracy of about \SI{99}{\percent} for words and only about \SI{5}{\percent} of the attempts resulted in no detection. Mehdi et al. use only one flexion sensor per finger and two IMUs to track the orientation of the hand. They only achieved an accuracy of \SI{88}{\percent} and some gestures could not be detected, because the movement of the forearm was not tracked. To improve their systems, the two groups state to investigate more on the algorithm and to use gloves with a higher reliability and accuracy.
