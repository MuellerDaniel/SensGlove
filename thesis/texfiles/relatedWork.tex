
\lhead[\chaptername~\thechapter]{\rightmark}

\rhead[\leftmark]{}

\lfoot[\thepage]{}

\cfoot{}

\rfoot[]{\thepage}


\chapter{Related Work}
\label{cha:relatedWork}

%\section{Abstracts of papers}
%\begin{itemize}
%\item \cite{huang2008pianotouch}
%\item \cite{huang2010mobile}
%\item \cite{metcalf2013markerless}
%\end{itemize}

\section{Approaches for hand motion reconstruction} \label{sec:approaches}

\cite{sturman1994survey}, \cite{dipietro2008survey}

\subsection{Vision/Camera based} \label{subsec:approaches:vision}
% General part about vision based motion estimation
Vision based motion capturing systems are widely used. They consist of one ore multiple cameras, arranged in a certain configuration, to generate quite exact replication of what wants to be tracked. Those systems are nowadays not only used for tracking and analyzing the motion of humans. The systems and applications range from general purpose devices for entertainment, like interacting with video games (Microsoft Kinect \todo{cite!}) or for examining the movement of athletes. So in the end it is no wonder that some groups came up with using a vision based system for hand motion reconstruction, even though those movements bring in some challenging aspects to consider. However the quality of the results of such vision based system is very high and can be classified as ground truth for motion estimation of fingers. It is very hard to manually reconstruct and measure the real values of finger angles and hand motion, since one can not see the bones without an x-ray. \\
% Steps and 3d model description
No matter what kind of vision based motion tracking system is used, to extract the actual hand pose and movement from an image, one has to perform the following steps:
\begin{enumerate}
\item Image fusion (if more than one camera is used) and preprocessing
\item Image processing, to get a focus on the relevant sections (the region of interest - ROI) 
\item Pose estimation, to extract and calculate the actual body pose from the image
\end{enumerate}
A fundamental part for these steps is providing a proper three dimensional object model. No matter whether one tracks the whole body or only a small section, the more detailed the outcome of the system should be, the more detailed has to be the model. The model uses a mesh of triangles and vertices and applies (anatomical) constraints on them. After extracting the relevant sections from the camera image, the model calculates and maps the actual positions and relations between the joints and bones. This step can consume a lot of computation time, if the result should be as detailed as possible. \cite{yun2013accurate} for example solved this problem, by combining a system identification stage, which uses the hand model, with a state estimation stage, where they use an Extended Kalman Filter.

For reconstructing the hand motion with a vision based system, one can find two approaches in literature: The tracking of markers, placed on the hand or a textile glove and the markerless detection of palm and fingers. At first a short overview on the marker based systems is presented. \\
% Marker approach
\cite{supuk2008evaluation}, \cite{metcalf2008validation} use passive reflective markers attached to the hand. While the Optotrak system \cite{optotrak}, used by \cite{supuk2008evaluation}, only needs one camera instance, the Vicon system \todo{cite!} comes with 6 cameras. Comparing the price \todo{add price here!} of those systems and the capabilities, measuring only the movement of small hands seems to break the relations. The accuracy of the outcome is directly dependent on the number and positions of used markers. \cite{metcalf2008validation} modeled the movement of wrist, hand, finger and thumb. Therefore they compare the results for different test persons, each equipped with 26 markers in total for one hand. The passive stickers are placed at the 3 knuckles of each finger, the fingertips and on the back of the hand and lower forearm to guarantee a tracking of the whole hand motion and not only of the fingers. It is very important, to place the markers for each person at the same anatomical positions. Attaching the reflectors statically on a textile glove would make the system more flexible and easier to use, but this would also lead to a degradation of the results. Every person's hand varies not only in size, but also in the position and lengths of the individual bones and knuckles. So a general purpose glove is very hard to construct. One additional issue that \cite{metcalf2008validation} came up with is, that one has to take the size of the surface area into account, too. The hands of children for example won't have the surface to place all markers at the desired positions properly. The placement of the stickers took them between 3 and 5 min each time and the results are very exact. In fact the aim of their study was to show that persons perform specific tasks in their own way, but that one can still observe similarities.
\cite{supuk2008evaluation} used the camera system more or less only as ground truth, to validate the data from a flexion based Data Glove. They use 19 passive markers, placed in a similar shape than the other group.\\
The group of \cite{yun2013accurate} are using active LED markers. Their paper emphasizes on an effective system identification algorithm and filtering method. They estimate one index finger with seven markers on it, recording it with a Phasespace Inc. system. They verified their reached accuracy in measurements by comparing the results to an optimized kinematic model.\\
The group of \cite{Wang:2009:RTH} use a multi-colored glove for finger identification. Their glove is printed with a special color pattern, to simplify the pose estimation problem. This allows them to use a single normal color camera, which is way cheaper than the motion capturing systems mentioned beforehand. The pose estimation is done upon a database, containing the glove in different articulations. The image gets processed, to extract the colors clearly, and the pose is found by a nearest neighbors approach, comparing to the database. To penalize the difference between the image and the matched pose from the database, they tune the result by applying inverse kinematics. In the end their systems works reliably. But it is only applicable for one glove size and the results are based on a single test person.\\

For all the above approaches, it is very important to place the markers correctly. So the system presumes that the user knows how to attach the stickers or wear the glove and which mistakes can be made. In order to facilitate this process and make it less fault-prone, a markerless approach would be a better choice.\\
By realizing such a variant however, the region of interest, so to say the actual position of the hand, is not directly given. Further on, the orientation and alignment of the test person has to be interpreted. The following section presents some implementations.\\
For image acquisition \cite{ionescu2005dynamic} use a gray scale camera and filter the image for the biggest white region. To get the best results they mention that one has to hold the hand in front of a black surface. In the end, they only try to detect certain hand gestures and not a whole motion. The images are compared to a pre-learned training set in order to recognize the poses. So this group doesn't use any models.\\
\cite{metcalf2013markerless} and \cite{sharp2015accurate} use the Microsoft Kinect. This system tries to define anatomic landmarks, to identify certain points of the hand. Actually the sensors are designed for declaring landmarks on the whole body, so for tracking a petite hand this identification process has to adopted. \cite{metcalf2013markerless} use the binary depth image and define the landmarks by searching for reasonable maxima and minima, with a 3d hand model. The model simulates the possible poses of the hand and matches them to the image. In the end their approach led to an overall accuracy of 78\%. A marker based system served as a comparison. The approach of \cite{metcalf2013markerless} was only tested with persons sitting on a table, so it is designed for a front-facing close-range scenario.  \cite{sharp2015accurate} go one step further and bring this system to a universal surrounding. They are able to extract the hand posture and movement from an arbitrary image, no matter at which distance the hand is or what the background looks like. The approach is to introduce a robust reinitializer who can cope with occlusion and image loss. In combination with a fast and effective comparison to a 3d hand model and a learned training data set, the movement and pose of the hand can be estimated very reliably, independent from the person or the environmental circumstances.\\
\cite{john2006advanced} uses two high resolution color cameras from Sony. To reconstruct the motion of the extracted human hand, the system compares the images to data from the 3d hand model. By matching the model to the input pictures, the position and configuration of the hand is estimated in real time.\\
The commercially available Leap Motion system \cite{leap} includes two IR cameras and 3 IR emitters and is particularly constructed for hand motion reconstruction. The Leap Motion Inc. provides a well documented API and software tools for Windows and Mac systems to use their device for basic interaction with a PC. The system directly outputs hand- and fingerpositions. It can also detect whether one is holding a pen or specific tool. Via the API one can directly access the absolute positions of the hand and joints. Also specific gestures, like swiping or drawing a circle with a finger get directly detected. \cite{weichert2013analysis} analyzed the accuracy and robustness of the Leap system. Different motions and positions were examined. As a \''test person\'' they choose an industrial robot with a position accuracy of 0.2 mm. The overall error of the system for dynamic motions was below 0.7 mm, which is better than the Microsoft Kinect system.\\
The Digits system, developed by \cite{Digits}, is a wrist wearable IR sensor. It consists of an IR laser line generator, a ring of modulated IR LEDs, a IR camera, and an IMU. The system collects on the one hand a single 3d point for each finger from the line generator, on the other hand a uniformly illuminated image of the hand, produced by the modulated IR LEDs. With those informations and by using forward or inverse kinematics of the underlying human hand model, the group can robustly reconstruct inward hand and finger movement. The IMU can track the alignment and movement of the whole forearm. The overall accuracy of the system is $ \leq $ 9° for the joint angles. This value varies with the fingers, since the thumb has a more complex movement and is smaller than the index finger for example. However these values satisfy the clinical standards for joint measurement.\\

Concluding the presented techniques for hand motion reconstruction by a vision based system, one could assert the following basic characteristics of such systems. (The impact or applicability of each point varies with the system, of course):
\begin{itemize}
\item The quality and stability of the tracking is limited to light conditions.
\item Occlusion of unseen fingers, for example by crossing or making a fist, can occur. Also clothes or body parts, held in front of the camera can hide parts of the hand.
\item The proposed systems are usually quite big or even need multiple cameras.
\item This induces that the installation has to be static and is only capable of local and no wearable tracking.
\item A three dimensional model of the human hand gets adopted to the images.
\item The algorithms for motion tracking are quite complex and are running on an external PC.
\item For marker based systems: The placement of the markers is crucial.
\end{itemize}


\subsection{IMU based} \label{subsec:approaches:IMU}
Another concept of motion tracking consists in using IMUs. Those sensors measure the angular rate, acceleration and magnetic field for three dimensions in space, they are also called 9-DOF sensors. With existing suitable algorithms, like the Madgwick filter \cite{madgwick2010efficient} one can calculate out of this data the absolute orientation of the sensor, relative to the earth magnetic field. So one can track instantaneous the orientation and direction of motion of a sensor unit. Therefore it is no surprise that those sensors are commonly used for motion tracking applications in general. The Dutch company Xsens Technologies, for example is specialized on motion tracking and develops several suits to track the whole body motion.\\
\cite{kortier2012ambulatory} use in \cite{kortier2012ambulatory}, \cite{kortier2014assessment} a self designed IMU system, consisting of 18 sensor units in total. The units are a gyroscope-accelerometer combination and placed on each proximal, intermediate and distal phalanges of each finger (for explanation of the bones see \todo{chapter!}). For additional information 3 units are placed on the back of the hand. The PCBs on the fingertips and on the dorsal side are additionally equipped with a magnetometer, to get a more accurate estimation about the orientation of the hand. To filter, estimate and map the raw sensor data to an adequate biomechanical hand model the group uses an Extended Kalman Filter (EKF) framework. They achieved an adequate repeatability. By comparing their approach to a vision based one, a maximum error of 12.4 mm was achieved. This value seems pretty high, but they claim that it is because of an misalignment between the optical and their coordinate frame. This shows once again, that the calibration plays an important role for vision based systems and is not so easy to manage.\\
The group of \cite{fang2014novel} use a similar method then \cite{kortier2012ambulatory}. The 16 IMUs, which are all full 9-DOF units, are also placed on the three bones of each finger. For the palmar movement however they use only one sensor. The processing of the data is also done \''on-hand\'' with the processor-board. For data filtering and position estimation they also use a Kalman Filter and a hand model. They characteristic lies in the evaluation of the sensor values. Because the hand is composed of rotational joints, they assume that either all sensors are in rotation or none. So they neglect the measurements of the gyros, if the hand is held still and only the fingers are moving, such that they only take the accelerometer and magnetometer data into account. However when the hand is moving, the measurements of the accelerometer and magnetometer have lower dynamics and they use the advantage of the gyros. Further on, they first estimate the pose of the palm, then the attitude of the proximal finger bones, then the angles of the DIP and PIP and finally they calculate the full hand pose, based on those intermediate results. In the end they achieved the requirements and point out that the efficiency of their method is almost twice that of the original EKF. Unfortunately they don't provide exact numbers.
There also exist some commercially available IMU based glove systems. The company Synertial or Anthrotronix for example provide ready to use gloves. The IGS-Glove from Synertial comes in various editions, differing in the number of sensors. It is available with 7, 12 or even 15 IMUs, mounted on the easy to wear glove, delivering you the desired accuracy \cite{Synertial}. Anthrotronix however equip their ''Acceleglove'' with 6 IMUs. Both deliver their systems with a SDK to have direct access to the raw sensor data but also to pre-calculated motion and gesture data.\\

Again, all the presented systems show some similarities. The following points try to summarize them:
\begin{itemize}
\item The IMUs are mounted on a textile cloth
\item However a unified cloth, that fits every human hand is difficult to produce
\item The accuracy varies with the number and placement of the sensors.
\item The more sensors used, the more wires are needed. Also the data traffic and processing time increases with the number of units.
\item A calibration procedure is needed, to increase the accuracy.
\item IMUs are cheap and available in a large variety, concerning the measurement resolution and 
\end{itemize}


\subsection{Flexion based} \label{subsec:approaches:flexion}
Another approach of measuring the hand movement is to monitor the flexion of fingers. There are different kinds of flexion sensors out there and many researchers use them for finger tracking. For example in 1977 Thomas de Fanti and Daniel Sandin developed one of the first data glove prototypes at the Massachusetts Institute of Technology (MIT). The Sayre Glove  \cite{sturman1994survey}. They equipped a glove with flexible tubes for each finger. At one end of each tube, they put a LED as light source and at the other end a photocell. The amount of light, arriving at the sensor varies with the flexion and extension of the finger. The more the finger is bent, the less light comes to the sensor.\\
Ten years later, in 1987 Visual Programming Language Research, Inc. rolled out some kind of successor to the Sayre Glove. Their device is equipped with five to ten flexion sensors, based on optical fibre \cite{zimmerman1985optical}. For more accuracy they place a flex sensor on each joint, to measure its angle. They even proposed a system with more sensors, to measure abduction and adduction between adjacent fingers.\\
Another way to measure the flexion are resistive or capacitive bend sensors. These devices can be printed with resistive ink and are therefore highly customizable in shape and size. Resistive bend sensors are used for example by \cite{o2013novel}, \cite{zecca2007development} or \cite{FifthDimension}. The Didjiglove \cite{sturman1994survey} in contrast is based on capacitive bend sensors.\\
The Italian company Gloreha \cite{Gloreha} follows a more application specific approach. Their rehabilitative glove system consists of mechanical cables for each finger. You can measure how much a finger is bended by the amount of wire extended. On the other hand you also can support the patient by extending or contracting the wire mechanically. This system is big, unhandy and looks more like an exoskeleton, than an unimpressive wearable. Of course it is constructed for rehabilitation and aimed to support specific motions of a patient and not for general purpose measuring of flexion and extension in every day life. But it still shows a mentionable approach.

In the end, one can say that flexion based hand tracking have the following characteristics:\\
\begin{itemize}
\item The sensors are mounted on the joints. Most groups use therefore a textile glove. 
\item The output of the system is dependent on the positions of the sensors. Ideally this should not change with the user. However each human hand is slightly different and there is not a universal glove size and sensor positioning, which would fit for all.
\item One way to improve this is to calibrate the glove system for each user.
\item The accuracy of the reconstructed finger positions or gestures is limited to the number and the measurement range of the used sensors. With one bend sensor per finger, one could at most only reconstruct the intention of the user's gesture or distinguish between several postures. However by introducing multiple sensors per finger, ideally more than one per joing, one could get an acceptable result. \cite{zecca2007development} used 15 bend sensors on a flexible PCB and reached an averaged error 7.1 deg \todo{unit} compared to a camera system.
\item One measures only the bending of a joint or finger. In order to reconstruct a relative or absolute position of the finger several calculations have to be made.
\item It is a simple and highly customizable system.
\item Easy applications can be realized with only a few sensors \todo{(see section applications for it)}
\end{itemize}


\subsection{Magnetic/electromagnetic based} \label{subsec:approaches:magnetic
Another approach is the use of measuring active and passive magnetic fields.\\
\cite{hashi2006wireless} are using an active, resonator based system. Their system consists of a driving coil, a pick up coil array and resonant LC markers. The markers, consisting of an inductive coil and a chip capacitor, are placed on the fingertips and have different resonant frequencies. The exciting coil modulates several signals and sends them out. An electromagnetic field is generated around the coil. Now, holding a marker inside this field the electromagnetic circuit begins to oscillate and generates his own resonant electromagnetic field. This can be measured by the pick up coil array. Each marker has a unique excitation frequency, so by modulating the received signals via FFT, the markers can be identified. Further on, the measured amplitude of the signal represents the intensity of the magnetic field. \cite{hashi2006wireless} assume that the excited field of the marker behaves like a magnetic dipole field. So by using the equation for a magnetic dipole one can calculate the position and orientation of each marker uniquely. They tested their approach and came up with a position accuracy up to 2 mm, for locations up to 100 mm away from the pick up coil array. Increasing the distance further, the results get worse. The group of \cite{schaffelhofer2012new} tested a commercially available system of Northern Digital \cite{wave} with primates. They achieved an overall accuracy of $ 2.41° \pm 3.36° $ for the tracking of dynamic movements. However the system, consisting of very complex and bulky components and is therefore not so well suited for a general purpose use.\\
The group of \cite{ma2010magnetic} take the approach of determining the position of a passive cylindrical bar magnet by approximating it with the magnetic dipole field. They tried to reconstruct the movement of the fingers, by placing neodymium magnets on the fingertips and measuring the magnetic field. Like the active approaches, they use the model for the magnetic dipole, to estimate the position and orientation of the passive magnets. To conclude from those estimated values to the actual finger position, they use an inverse kinematic approach with and underlying human hand model. For verification of this approach they equipped a test person with one magnet on the index fingertip and 6 sensors on a wristband. The proband was asked to perform several flexion and extension tasks of his finger. The results for the estimated finger positions and orientations were consistent with the data recorded by a Vicon system. Exact accuracy values are not provided by the group.\\
In the end one can say, that this approach sounds promising. \cite{ma2010magnetic} propose a system, that is wearable and easy to use.


\subsection{Other approaches} \label{subsec:approaches:other}
As one can see, there is a lot of research going on in the area of motion estimation for the human hand. The focus of the so far presented approaches was relied to general purpose devices, designed for trying to track reliably and accurate the whole range of possible motions. Due to the wide range of application possibilities (see~\ref{sec:applications}) there are also some more specialised variants in reconstructing gesture or movement recognition of the hand. This section focuses on some of them.\\
The Pinch Gloves, designed by Fakespacelabs \todo{Every company with a link to their site???} represents an input device. The system consists of a glove and conductive elements, sewn into the tips of each of the fingers. When two or more fingers are pinched together, the conductive parts come into contact and generate an individual signal. This signal can easily be interpreted by a computer and therefore serve as an input. There is also the possibility to attach a position tracker and add the motion of the hand as an input possibility. The system is used for virtual environment interaction. In \cite{bowman2001using} a more elaborated interaction technique with this simple system is presented. They developed an environment to navigate through menus or to type on a virtual keyboard.\\
The eRing, developed by \cite{wilhelm2015ering} is another kind of gesture interaction device. It consists, as the name suggests, of a ring, enclosed by capacitive foils. The capacity of the system is related to the conductive environment. As the human body has an influence on that the capacity changes by moving the fingers around the ring. This change in capacity can be measured, by determining the rise time $ \tau $ of the RC circuit. In their paper the group describes that the system is able to detect static and dynamic gestures, as long as the neighbouring fingers are not to far away from the ring. For recognizing the gestures they use a 1-nearest neighbour approach on a pre learned dataset.\\
The Rutgers master {2} represents an exoskeleton like approach \cite{bouzit2002rutgers}. It consists of four pistons with rings on the ends, to clip them on the thumb, index, middle and ring finger. The movement of the pinky finger is neglected. The adduction and abduction of the fingers is measured by Hall-effect sensors, the flexion and extension by infrared sensors. From the piston movement one can calculate the finger angles via a kinematic hand model. The pistons are inside an air cylinder to reduce friction in the system. This glove can provide force-feedback to the fingers, since the pistons can be controlled externally. Its main application area is therefore the rehabilitation and learning of hand movements. However the exoskeleton structure restricts the range of movement. Only 55 \% of the natural grasping motion can be performed with this system. The accuracy of the system was evaluated to $ 0.75\deg  $ for the adduction/abduction and 0.5 mm for the piston position. A similar system is the CyberGrasp haptic glove \cite{cyberglove}. It is slightly more accurate but also heavier and even more cumbersome. Also the italian company Gloreha focuses on force-feedback gloves for rehabilitation \cite{Gloreha}.\\
An interesting approach is mentioned in \cite{mascaro2001photoplethysmograph}. They introduce a sensor placed onto the fingernail, measuring emitted light. LEDs are placed on top of the nail, emitting light with different wave lengths and measure the response from the nailbed. This technique is called reflectance photoplethysmography. If there is a force applied to the fingertip, one can see that the color of the skin beneath the nail changes. This behaviour can also be obtained when moving the finger. Those, often very small and not clearly visible, changes in color can be detected by their system. However there are many factors to take into account like skin color, blood flow in the fingers, texture of skin and so on, which complicate the reconstruction of motion. Hence they are only able to measure forces with this system.\\
Another approach for recognizing gestures is by measuring the electric potential of forearm muscles \cite{kim2008emg} By performing gestures with the hand, the electromyographic potential especially in the forearm changes. This can be recorded by a so called electromyograph (EMG). This approach is based on a learning data set, recorded for one person and by recognizing those gestures in real time. \cite{zhang2011framework} designed a framework for this, taking also the data of an accelerometer into account.

\section{Possible fields for applications} \label{sec:applications}

\subsection{Human-Computer-Interface (HCI)} \label{subsec:applications:HCI}
The way we interact with electronic devices becomes more and more natural and is still changing. In the recent years touch input became ubiquitous, for example. Gaming consoles already bring the possibility to physically interact with the games. The Kinect camera for Xbox, the Nintendo Wii controller or other haptic systems. Hand motion based systems could be one way to bring the human computer interaction to the next level. Commercially available devices like the Leap Motion or the Myo wristband allow a broad gesture based interaction with the computer or smartphone. The Leap controller for example enables you to play games or interact with 3d graphics in a natural way. The benefit of such hand tracking devices is, that you don't need a big camera set up and don't have to leave your desk position. You just place the device in front of you or wear it, perform the desired gesture and the system behaves as you want. For example by swiping a window gets closed or the field of view gets enlarged by pinching two fingers. For the interaction with devices, such as smartphones, the system has to be wearable, like in \cite{Digits}. Also the evolving field of virtual reality environments serves as a base for hand motion interaction. For the Leap motion again, there exists a mount to combine it with the Oculus Rift. Also creative tasks can be performed by a hand motion tracking system, for example drawing and designing a 3d object with your fingers on a virtual canvas.\\
Another, a bit more serious, way for HCI is the control of robotic machines, especially arm like devices. Such an application could be safety critical, so the interpretation of the hand motion has to be accurate and reliable, like the system proposed by \cite{sharp2015accurate}. Dependent on the application those devices should be able to track the whole finger movement and not only predefined gestures. Examples could be controlling robots in space \cite{dipietro2008survey} or in dangerous environments like military or for bomb disposal \cite{greenleaf1996developing}. Also the execution of surgical tasks could be one field of application. Nowadays invasive operations should be performed in no time and leave very small scars. For that a lot of endoscopic surgery is done with remote controlled catheters. However most of those invasive devices have very low functionality, often limited to cutting or exhausting something. The devices are controlled by the surgeon often by a simple mechanical system. Giving the practitioners the ability to use a more sensitive and complex method of interaction, would not only reduce the time needed for the intervention and the risks for the patient but also enlarge the possibilities of interventions.\\
\todo{A concluding sentence??? Saying sth. like \"... one can see that the accuracy and mobility is dependent on the application...\". But this is obvious and in some sense stated above...}

\subsection{Therapeutic/Rehabilitation} \label{subsec:applications:reha}
The exact measurement of finger joints, called goniometry, is still a time consuming and error-prone task. Therapists and doctors have to measure patient's range of movement (ROM) in case of hand immobility diseases like arthritis, rheumatism \cite{o2013novel} or parkinson’s disease \cite{su20033}, or after a hand operation or fraction. Till now they use mechanical goniometers \cite{williams2000goniometric} to measure static angle positions, dynamic measurements are not possible. A more accurate and reliable method would ease their life tremendously. This is where the hand motion reconstruction comes in. As described in section \ref{sec:approaches} there are already systems which can measure the ROM of fingers very accurate. \cite{williams2000goniometric} verified their flexion based glove as clinically admitted and can even measure adduction/abduction more exactly than with goniometers. The group of \cite{o2013novel} developed a flexion and IMU based glove with a very detailed interface for doctors. They can record and visualize raw sensor data, like the bending angle or movement velocity, as well as a 3d hand, miming the actions of the user. The interface can also provide information about former measurements and can therefore monitor the development of the patients moving behaviour. The diagnosis and measurements for Parkinson's disease are also critical. Till now the diagnosis relies on objective observations by the doctor and patients. This induces that the illness is often detected in an advanced stage. \cite{su20033} developed a system especially to detect this neurological disease. With the help of a electromagnetic based glove several, Parkinson critical experiments where carried out. In the end they compared the results for ill and healthy patients and could clarify significant differences in the execution of the tasks.\\
Another possibility to support especially the work of therapists is the home-based rehabilitation or telemedicine \cite{metcalf2013markerless}. This means that the patients have a suitable guidance system at home and can perform the exercises with it. The paper of \cite{durfee2007technical} validated that such systems can return the same results as clinical tests and could therefore save the patients time and effort. The system could for example detect over- or under-exertion while performing a presented task. Further on, by providing an attractive interface like a virtual-reality environment, the patients are much more motivated to execute the training on their own \cite{popescu2000virtual}. Such an interface doesn't has to be complex. The patient could see for example virtual objects on a screen and try to interact with them. \cite{popescu2000virtual} developed a system for the hand, using the Rutgers Master \rom{2}, which also uses force feedback to train the patient. The environment, consisting of a PC and the glove system, records not only the performed motions but also takes a video of the exercise session and can directly send them to the therapist. This ensures a constant verification and interaction with the clinic personal. That such a system can improve the work of therapists is validated in \cite{heuser2007telerehabilitation}. In their study five postsurgery subjects suffering from Carpal tunnel syndrome were trained to perform tasks. The effects in hand function improvement was tremendous for all subjects. The strength for grip and pinch movement increased up to 150\%. The system was very good accepted by the patients and the therapists.\\ 
However for such a telemedicine system the accuracy has to be exact and reliable, also the data has to be processed in real time. One of the most important requirement is however the usability. The system has to be easy to set up and able to detect false focuses or attitudes. A glove based system for example brings in difficulties in donning and doffing \cite{metcalf2013markerless}, other systems are cumbersome or have fragile parts like wires \cite{bouzit2002rutgers}, which complicate the usage. For camera based systems the focus, illumination and the background of the image are critical points \cite{ionescu2005dynamic}.\\
Till now only the mentioned studies and systems, described in \cite{heuser2007telerehabilitation} and \cite{popescu2000virtual} where performed, but certainly there will be more investigation in the near future.


\subsection{Activity and Gesture tracking} \label{subsec:applications:activity}
As already depicted in section \ref{subsec:applications:HCI}, the reconstruction of human hand motion can be used for gesture recognition. Activity recognition is mainly based on using a training data set for the gestures and an algorithm, to compare the actual movement with this database and finally judging whether the gesture has been performed or not. But this feature can not only be used as an HCI. Having a wearable system one can detect specific gestures ubiquitously, which can be used to produce a diary like tracking of hand gestures. Such a monitoring system could be a support again for therapists, trying to analyse the daily routines of their patients. It is possible to detect a grasp intention \cite{supuk2008evaluation}, \cite{zhang2011framework} and also the strength of a grasp \cite{ekvall2005grasp}, so by downsizing the systems one could get a detailed logbook about ones hand activities. The eRing \cite{wilhelm2015ering} could be such a system. With an unimposing data glove system, like \cite{FifthDimension} or \cite{Synertial}, one could even track the whole hand motion. Because of the mentioned reasons to use such a system in a ubiquitous environment, vision based systems are not so well suited for this application field. Apart from the Digits \cite{Digits} system, most of the other developments use a bigger camera system.\\
Another possible application field concerning the recognition of gestures is the understanding and translation of sign language. Deaf persons are using the sign language to communicate with the outside world. However most of the \''talking\'' persons do not know this complicated alphabet based on specific hand gestures and poses, such that it can be very tiring for a deaf person to interact with others. Several groups have done research in this field like presented in \cite{mehdi2002sign}, \cite{fels1993glove} and \cite{dipietro2008survey}. Again, a wearable and mobile system is better suited for this task than a static camera based approach. As the sign language has a lot, but sometimes similar looking, gestures the training data set and the corresponding classification algorithm have to be fast and reliable. In the end such a hand tracking system has to be reliable, accurate, and able to track the movement of the whole hand. As a classification algorithm \cite{fels1993glove} use neural networks. The detected letters or words can then be visualized on a display or directly made audible by speakers \cite{fels1993glove}. To ensure a natural behaviour, the gestures have to be interpreted and visualized in real time. \cite{fels1993glove} used a fibre optic data glove with 11 sensors, including an IMU to track the heading of the hand. In the end they achieved an accuracy of about 99 \% for words and only about 5 \% of the attempts resulted in no detection. \cite{mehdi2002sign} use only one flexion sensor per finger and two to track the orientation of the hand. They only achieved an accuracy of 88 \% and some gestures could not be detected, because the movement of the hand was not tracked. To improve their systems, the two groups state to investigate more on the algorithm and use more accurate gloves.
